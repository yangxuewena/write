\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{times}


\cvprfinalcopy % *** Uncomment this line for the final submission
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\usepackage{indentfirst}
\setlength{\parindent}{2em}
\usepackage{cite}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green,backref=page]{hyperref}

\author{Xuewen Yang\\\\
August 17 2018}

\title{Scale-Transferrable Object Detection}
\begin{document}
\maketitle
\begin{abstract}
Scale problem lies in the heart of object detection. In this
work, the author develop a novel Scale-Transferrable Detection
Network (STDN) for detecting multi-scale objects in images.
In contrast to previous methods that simply combine
object predictions from multiple feature maps from different
network depths, the proposed network is equipped with
embedded super-resolution layers (named as scale-transfer
layer/module in this work) to explicitly explore the interscale
consistency nature across multiple detection scales.
Scale-transfer module naturally fits the base network with
little computational cost. This module is further integrated
with a dense convolutional network (DenseNet) to yield a
one-stage object detector. They evaluate this proposed architecture
on PASCAL VOC 2007 and MS COCO benchmark
tasks and STDN obtains significant improvements over the
comparable state-of-the-art detection models.
\end{abstract}
\section{Introduction}
Scale problem lies in the heart of object detection. In
order to detect objects of different scales, a basic strategy
is to use image pyramids to obtain features at different
scales. However, this will greatly increase memory and
computational complexity, which will reduce the real-time
performance of object detectors.

Scale problem lies in the heart of object detection. In
order to detect objects of different scales, a basic strategy
is to use image pyramids to obtain features at different
scales. However, this will greatly increase memory and
computational complexity, which will reduce the real-time
performance of object detectors.

In recent years, convolutional neural networks(CNN) have achieved great success in computer vision tasks,
such as image classification\cite{Krizhevsky2012ImageNet}, semantic segmentation
\cite{Long2015Fully}, and object detection. The hand-engineered features
are replaced with features computed by convolutional
neural networks, which greatly improves the performance
of object detectors. Faster R-CNN uses convolutional
feature maps computed by one layer to predict candidate region
proposals with different scales and aspect ratios. Because the receptive field of each layer in CNN
is fixed, there exists inconsistency between the fixed receptive
field and the objects at different scales in natural images.
This may compromise object detection performance.
SSD and MS-CNN use feature maps from different layers within CNN to predict objects at different scales. Shallow feature maps have small receptive
fields that are used to detect small objects, and deep
feature maps have large receptive fields that are used to
detect large objects. Nevertheless, shallow features have
less semantic information, which may impair the performance
of small object detection. FPN, ZIP and
DSSD integrate semantic information on feature maps
at all scales. As shown in Figure~\ref{fig:1}, a top-down architecture
combines high-level semantic feature maps with lowlevel
feature maps to yield more semantic feature maps at
all scales. However, in order to improve detection performance,
feature pyramids must be carefully constructed, and
adding extra layers to build the feature pyramids brings additional
computational cost.

In order to obtain high-level semantic multi-scale feature
maps, and also without impairing the speed of the detector,
we develop a Scale-Transfer Module (STM) and embed
this module directly into a DenseNet\cite{Huang2017Densely}. The role
of DenseNet is to integrate low-level and high-level features
within a CNN to get more powerful features. Because
of the densely connected network structure, the features of DenseNet are naturally more powerful than the ordinary
convolutional features. STM consists of pooling and scaletransfer
layers. Pooling layer is used to obtain small scale
feature maps, and scale-transfer layer is used to obtain large
scale feature maps. Scale-transfer layer is first proposed to
do image super-resolution because of its simplicity and
efficiency, and some people also use it to do semantic segmentation. We use this layer to efficiently expand the
resolution of the feature map for object detection.


\begin{figure}
\centering
\includegraphics[width=8cm,height=6cm]{1}
\caption{Importance of context for object recognition. Without the context (face), it is hard to recognize the black curve in the middle area as a nose.}
\label{fig:1}
\end{figure}
\section{Related Work}
State-of-the-art methods of object detection are based
on convolutional neural networks. For example, SPPnet, Fast R-CNN, Faster R-CNN, R-FCN, and
YOLO use features from the top layer of the convolutional
neural network to detect objects of different scales.
However, since each layer of the convolutional neural network has a fixed receptive field, it is not optimal to predict
objects of different scales with only features of one layer.

There are generally three main types of methods to further
improve the accuracy of multi-scale object detection.
One is to detect objects using the combinations of multilayer
features. The other is to use different layer features to
predict objects at different scales. The last is a combination
of the above two methods.

For the first type of method, ION uses skip pooling
to extract information at multiple layers, and then the object
is detected by using the combined features. HyperNet incorporates deep, intermediate and shallow features
of the image for generating proposals and detecting objects.
YOLO concatenates the higher resolution features
with the low-resolution features by passthrough layer and
runs detection on top of this expanded feature map. The basic
idea of these methods is to enhance the power of features
by combining low-level and high-level features.



{\small
\bibliographystyle{ieee}
\bibliography{1}
}

\end{document}
